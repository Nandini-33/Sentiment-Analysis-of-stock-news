{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d18f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c1c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "697aa222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>12:44PM</td>\n",
       "      <td>Ray Wang on Amazon.com Inc (NASDAQ:AMZN): Stro...</td>\n",
       "      <td>ray wang amazoncom inc nasdaqamzn strong funda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>12:06PM</td>\n",
       "      <td>Duck Capital calls for 'significant' capital r...</td>\n",
       "      <td>duck capital call significant capital return a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>12:00PM</td>\n",
       "      <td>Is an earnings beat enough for Big Tech invest...</td>\n",
       "      <td>earnings beat enough big tech investor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>11:37AM</td>\n",
       "      <td>Amazon pilots 'Rufus' generative AI shopping a...</td>\n",
       "      <td>amazon pilot rufus generative ai shopping assi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>11:16AM</td>\n",
       "      <td>Do Amazon, Alphabet, and Apple Have an AI Spen...</td>\n",
       "      <td>amazon alphabet apple ai spending problem mean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date     time  \\\n",
       "0   AMZN  Oct-29-24  12:44PM   \n",
       "1   AMZN  Oct-29-24  12:06PM   \n",
       "2   AMZN  Oct-29-24  12:00PM   \n",
       "3   AMZN  Oct-29-24  11:37AM   \n",
       "4   AMZN  Oct-29-24  11:16AM   \n",
       "\n",
       "                                               title  \\\n",
       "0  Ray Wang on Amazon.com Inc (NASDAQ:AMZN): Stro...   \n",
       "1  Duck Capital calls for 'significant' capital r...   \n",
       "2  Is an earnings beat enough for Big Tech invest...   \n",
       "3  Amazon pilots 'Rufus' generative AI shopping a...   \n",
       "4  Do Amazon, Alphabet, and Apple Have an AI Spen...   \n",
       "\n",
       "                                       cleaned_title  \n",
       "0  ray wang amazoncom inc nasdaqamzn strong funda...  \n",
       "1  duck capital call significant capital return a...  \n",
       "2             earnings beat enough big tech investor  \n",
       "3  amazon pilot rufus generative ai shopping assi...  \n",
       "4  amazon alphabet apple ai spending problem mean...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles=df['title'].tolist()\n",
    "cleaned_titles=[]\n",
    "# Define the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocessing function\n",
    "for text in titles:\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "      \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize each word\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    \n",
    "    cleaned_titles.append(preprocessed_text)\n",
    "\n",
    "# Apply preprocessing to each title in the list\n",
    "\n",
    "# Add the cleaned titles back to the DataFrame\n",
    "df['cleaned_title'] = cleaned_titles\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "642b533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363b0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123902dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def sentiwordnet_score(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     pos_tags = nltk.pos_tag(tokens)\n",
    "#     sentiment_score = 0\n",
    "#     count = 0\n",
    "    \n",
    "#     for word, tag in pos_tags:\n",
    "#         # Get WordNet POS tag\n",
    "#         wn_tag = get_wordnet_pos(tag)\n",
    "        \n",
    "#         if wn_tag:  # Only proceed if POS tag is recognized\n",
    "#             # Lemmatize the word\n",
    "#             lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            \n",
    "#             # Get SentiWordNet scores\n",
    "#             synsets = list(swn.senti_synsets(lemma, wn_tag))\n",
    "#             if synsets:\n",
    "#                 # Use the first synset's score as a representative score\n",
    "#                 synset = synsets[0]\n",
    "#                 sentiment_score += synset.pos_score() - synset.neg_score()\n",
    "#                 count += 1\n",
    "\n",
    "#     # Calculate the average sentiment score for the text\n",
    "#     if count > 0:\n",
    "#         return sentiment_score / count\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5405328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assign_sentiwordnet_label(score):\n",
    "#     if score >=0:\n",
    "#         return 'positive'\n",
    "#     else:\n",
    "#         return 'negative'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57bdd42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['senti_score'] = df['title'].apply(sentiwordnet_score)\n",
    "# df['sentiment_label'] = df['senti_score'].apply(assign_sentiwordnet_label)\n",
    "\n",
    "# # Display the DataFrame with sentiment scores and labels\n",
    "# print(df[['title', 'senti_score', 'sentiment_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fe7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = vectorizer.fit_transform(df['title'])\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "044e56f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  \\\n",
      "0    Ray Wang on Amazon.com Inc (NASDAQ:AMZN): Stro...   \n",
      "1    Duck Capital calls for 'significant' capital r...   \n",
      "2    Is an earnings beat enough for Big Tech invest...   \n",
      "3    Amazon pilots 'Rufus' generative AI shopping a...   \n",
      "4    Do Amazon, Alphabet, and Apple Have an AI Spen...   \n",
      "..                                                 ...   \n",
      "495  Microsoft Corporation (MSFT) Gave Back Some of...   \n",
      "496  Tesla's Stock Taps the Brakes, But It's Still ...   \n",
      "497  Betting on Bitcoin? Microsoft's shareholders w...   \n",
      "498  Colgate-Palmolive, Centene, Microsoft: 3 stock...   \n",
      "499  Microsoft CEO Satya Nadella asked for a pay cu...   \n",
      "\n",
      "     weighted_sentiment_score sentiment_label  \n",
      "0                    0.000000        positive  \n",
      "1                    0.073346        positive  \n",
      "2                    0.058731        positive  \n",
      "3                    0.000000        positive  \n",
      "4                    0.000000        positive  \n",
      "..                        ...             ...  \n",
      "495                  0.000000        positive  \n",
      "496                  0.000000        positive  \n",
      "497                 -0.050385        negative  \n",
      "498                  0.046563        positive  \n",
      "499                 -0.030970        negative  \n",
      "\n",
      "[500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def calculate_weighted_sentiment_score(text, tfidf_vector):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    weighted_score = 0\n",
    "\n",
    "    for idx, (word, tag) in enumerate(pos_tags):\n",
    "        # Get WordNet POS tag and lemmatize the word\n",
    "        wn_tag = get_wordnet_pos(tag)\n",
    "        if wn_tag:  # Only proceed if POS tag is recognized\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "\n",
    "            # Get SentiWordNet scores\n",
    "            synsets = list(swn.senti_synsets(lemma, wn_tag))\n",
    "            if synsets:\n",
    "                # Take the first synset as the representative score\n",
    "                synset = synsets[0]\n",
    "                sentiment_score = synset.pos_score() - synset.neg_score()\n",
    "\n",
    "                # Check if word exists in the TF-IDF feature names\n",
    "                if word in tfidf_feature_names:\n",
    "                    tfidf_index = tfidf_feature_names.tolist().index(word)\n",
    "                    tfidf_value = tfidf_vector[tfidf_index]\n",
    "\n",
    "                    # Multiply sentiment score by TF-IDF weight\n",
    "                    weighted_score += sentiment_score * tfidf_value\n",
    "\n",
    "    return weighted_score\n",
    "\n",
    "# Step 3: Apply the weighted sentiment score calculation for each title\n",
    "weighted_scores = []\n",
    "for i, title in enumerate(df['title']):\n",
    "    tfidf_vector = X_tfidf[i].toarray().flatten()\n",
    "    weighted_score = calculate_weighted_sentiment_score(title, tfidf_vector)\n",
    "    weighted_scores.append(weighted_score)\n",
    "\n",
    "# Add weighted scores to DataFrame\n",
    "df['weighted_sentiment_score'] = weighted_scores\n",
    "\n",
    "# Step 4: Assign labels based on the weighted sentiment score\n",
    "def assign_sentiment_label(score):\n",
    "    if score >= 0:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "\n",
    "df['sentiment_label'] = df['weighted_sentiment_score'].apply(assign_sentiment_label)\n",
    "\n",
    "# Display the DataFrame with scores and labels\n",
    "print(df[['title', 'weighted_sentiment_score', 'sentiment_label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37ca6855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_title     label\n",
      "0    ray wang amazoncom inc nasdaqamzn strong funda...  positive\n",
      "1    duck capital call significant capital return a...  positive\n",
      "2               earnings beat enough big tech investor  positive\n",
      "3    amazon pilot rufus generative ai shopping assi...  positive\n",
      "4    amazon alphabet apple ai spending problem mean...  negative\n",
      "..                                                 ...       ...\n",
      "495  microsoft corporation msft gave back first hal...  positive\n",
      "496  tesla stock tap brake still rising magnificent...  positive\n",
      "497  betting bitcoin microsofts shareholder decide ...  positive\n",
      "498     colgatepalmolive centene microsoft stock focus  positive\n",
      "499  microsoft ceo satya nadella asked pay cut stil...  negative\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Ensure that VADER lexicon is downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to assign sentiment labels\n",
    "def assign_sentiment(text):\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    if score >=0:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "   \n",
    "    \n",
    "\n",
    "# Apply the function to the 'cleaned_title' column\n",
    "df['label'] = df['cleaned_title'].apply(assign_sentiment)\n",
    "\n",
    "# Display the DataFrame with sentiment labels\n",
    "print(df[['cleaned_title', 'label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11971649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.886\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(df['label'], df['sentiment_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da4a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
