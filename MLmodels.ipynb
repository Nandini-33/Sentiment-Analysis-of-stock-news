{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d31e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d364c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bea4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>12:44PM</td>\n",
       "      <td>Ray Wang on Amazon.com Inc (NASDAQ:AMZN): Stro...</td>\n",
       "      <td>ray wang amazoncom inc nasdaqamzn strong funda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>12:06PM</td>\n",
       "      <td>Duck Capital calls for 'significant' capital r...</td>\n",
       "      <td>duck capital call significant capital return a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>12:00PM</td>\n",
       "      <td>Is an earnings beat enough for Big Tech invest...</td>\n",
       "      <td>earnings beat enough big tech investor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>11:37AM</td>\n",
       "      <td>Amazon pilots 'Rufus' generative AI shopping a...</td>\n",
       "      <td>amazon pilot rufus generative ai shopping assi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Oct-29-24</td>\n",
       "      <td>11:16AM</td>\n",
       "      <td>Do Amazon, Alphabet, and Apple Have an AI Spen...</td>\n",
       "      <td>amazon alphabet apple ai spending problem mean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker       date     time  \\\n",
       "0   AMZN  Oct-29-24  12:44PM   \n",
       "1   AMZN  Oct-29-24  12:06PM   \n",
       "2   AMZN  Oct-29-24  12:00PM   \n",
       "3   AMZN  Oct-29-24  11:37AM   \n",
       "4   AMZN  Oct-29-24  11:16AM   \n",
       "\n",
       "                                               title  \\\n",
       "0  Ray Wang on Amazon.com Inc (NASDAQ:AMZN): Stro...   \n",
       "1  Duck Capital calls for 'significant' capital r...   \n",
       "2  Is an earnings beat enough for Big Tech invest...   \n",
       "3  Amazon pilots 'Rufus' generative AI shopping a...   \n",
       "4  Do Amazon, Alphabet, and Apple Have an AI Spen...   \n",
       "\n",
       "                                       cleaned_title  \n",
       "0  ray wang amazoncom inc nasdaqamzn strong funda...  \n",
       "1  duck capital call significant capital return a...  \n",
       "2             earnings beat enough big tech investor  \n",
       "3  amazon pilot rufus generative ai shopping assi...  \n",
       "4  amazon alphabet apple ai spending problem mean...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles=df['title'].tolist()\n",
    "cleaned_titles=[]\n",
    "# Define the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocessing function\n",
    "for text in titles:\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "      \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize each word\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    \n",
    "    cleaned_titles.append(preprocessed_text)\n",
    "\n",
    "# Apply preprocessing to each title in the list\n",
    "\n",
    "# Add the cleaned titles back to the DataFrame\n",
    "df['cleaned_title'] = cleaned_titles\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af77b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Scores:\n",
      "      aapl  aapls  accelerating  access  according  account  accusing  action  \\\n",
      "0     0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "1     0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "2     0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "3     0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "4     0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "..    ...    ...           ...     ...        ...      ...       ...     ...   \n",
      "495   0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "496   0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "497   0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "498   0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "499   0.0    0.0           0.0     0.0        0.0      0.0       0.0     0.0   \n",
      "\n",
      "     actually   ad  ...  worth  would  wrap  wrestle  year  yield  youtube  \\\n",
      "0         0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "1         0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "2         0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "3         0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "4         0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "..        ...  ...  ...    ...    ...   ...      ...   ...    ...      ...   \n",
      "495       0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "496       0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "497       0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "498       0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "499       0.0  0.0  ...    0.0    0.0   0.0      0.0   0.0    0.0      0.0   \n",
      "\n",
      "     zelle  zone  zuckerbergs  \n",
      "0      0.0   0.0          0.0  \n",
      "1      0.0   0.0          0.0  \n",
      "2      0.0   0.0          0.0  \n",
      "3      0.0   0.0          0.0  \n",
      "4      0.0   0.0          0.0  \n",
      "..     ...   ...          ...  \n",
      "495    0.0   0.0          0.0  \n",
      "496    0.0   0.0          0.0  \n",
      "497    0.0   0.0          0.0  \n",
      "498    0.0   0.0          0.0  \n",
      "499    0.0   0.0          0.0  \n",
      "\n",
      "[500 rows x 1097 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Apply TF-IDF to the 'cleaned_title' column\n",
    "tfidf_matrix = vectorizer.fit_transform(df['cleaned_title'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for easier readability\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the TF-IDF DataFrame\n",
    "print(\"TF-IDF Scores:\\n\", tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a04f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         cleaned_title sentiment_label\n",
      "0    ray wang amazoncom inc nasdaqamzn strong funda...        positive\n",
      "1    duck capital call significant capital return a...        positive\n",
      "2               earnings beat enough big tech investor        positive\n",
      "3    amazon pilot rufus generative ai shopping assi...        positive\n",
      "4    amazon alphabet apple ai spending problem mean...        negative\n",
      "..                                                 ...             ...\n",
      "495  microsoft corporation msft gave back first hal...        positive\n",
      "496  tesla stock tap brake still rising magnificent...        positive\n",
      "497  betting bitcoin microsofts shareholder decide ...        positive\n",
      "498     colgatepalmolive centene microsoft stock focus        positive\n",
      "499  microsoft ceo satya nadella asked pay cut stil...        negative\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/nandiniupadhyay/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to assign sentiment labels\n",
    "def assign_sentiment(text):\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    if score >= 0:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "\n",
    "# Apply the function to the 'cleaned_title' column\n",
    "df['sentiment_label'] = df['cleaned_title'].apply(assign_sentiment)\n",
    "\n",
    "# Display the DataFrame with sentiment labels\n",
    "print(df[['cleaned_title', 'sentiment_label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d3b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['title'])  # Features (TF-IDF vectors)\n",
    "y = df['sentiment_label']  # Target labels (positive, negative)\n",
    "\n",
    "# Step 2: Split Data into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the SVM Model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predict and Evaluate\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18652390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.30      0.46        10\n",
      "    positive       0.93      1.00      0.96        90\n",
      "\n",
      "    accuracy                           0.93       100\n",
      "   macro avg       0.96      0.65      0.71       100\n",
      "weighted avg       0.94      0.93      0.91       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927fca78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Predict and Evaluate\n",
    "y_pred = dt_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1657b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "label_mapping = {'positive': 1, 'negative': 0}\n",
    "df['label'] = df['sentiment_label'].map(label_mapping)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences of tokens\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to ensure consistent input size\n",
    "max_length = 50  # max length for sequences\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98fef3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 1s 13ms/step - loss: 0.3765 - accuracy: 0.8625 - val_loss: 0.3290 - val_accuracy: 0.9000\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.3386 - accuracy: 0.8925 - val_loss: 0.3058 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2087 - accuracy: 0.9300 - val_loss: 0.3780 - val_accuracy: 0.8300\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1523 - accuracy: 0.9400 - val_loss: 0.3571 - val_accuracy: 0.8800\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1115 - accuracy: 0.9675 - val_loss: 0.4333 - val_accuracy: 0.8500\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0695 - accuracy: 0.9800 - val_loss: 0.3345 - val_accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0350 - accuracy: 0.9900 - val_loss: 0.3872 - val_accuracy: 0.8800\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0252 - accuracy: 0.9925 - val_loss: 0.3912 - val_accuracy: 0.8800\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0167 - accuracy: 0.9950 - val_loss: 0.4247 - val_accuracy: 0.8800\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.4491 - val_accuracy: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2860a25d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "# Define RNN model\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_length),\n",
    "    SimpleRNN(64),\n",
    "    Dense(2, activation='softmax')  # Output layer for 2 classes: positive, negative\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "rnn_model.fit(X_train_padded, y_train, epochs=10, batch_size=16, validation_data=(X_test_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "552192f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4491 - accuracy: 0.8900\n",
      "RNN Test Accuracy: 0.8899999856948853\n"
     ]
    }
   ],
   "source": [
    "# Evaluate RNN\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test_padded, y_test)\n",
    "print(\"RNN Test Accuracy:\", rnn_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "709ac40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "25/25 [==============================] - 1s 24ms/step - loss: 0.4661 - accuracy: 0.8575 - val_loss: 0.3259 - val_accuracy: 0.9000\n",
      "Epoch 2/7\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.3476 - accuracy: 0.8925 - val_loss: 0.3253 - val_accuracy: 0.9000\n",
      "Epoch 3/7\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.3468 - accuracy: 0.8925 - val_loss: 0.3259 - val_accuracy: 0.9000\n",
      "Epoch 4/7\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.3430 - accuracy: 0.8925 - val_loss: 0.3409 - val_accuracy: 0.9000\n",
      "Epoch 5/7\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.3439 - accuracy: 0.8925 - val_loss: 0.3241 - val_accuracy: 0.9000\n",
      "Epoch 6/7\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.3403 - accuracy: 0.8925 - val_loss: 0.3216 - val_accuracy: 0.9000\n",
      "Epoch 7/7\n",
      "25/25 [==============================] - 0s 14ms/step - loss: 0.1960 - accuracy: 0.9425 - val_loss: 0.3125 - val_accuracy: 0.9100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28c8aded0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Define LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_length),\n",
    "    LSTM(64),\n",
    "    Dense(2, activation='softmax')  # Output layer for 2 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_padded, y_train, epochs=7, batch_size=16, validation_data=(X_test_padded, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1724b2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3125 - accuracy: 0.9100\n",
      "LSTM Test Accuracy: 0.9100000262260437\n"
     ]
    }
   ],
   "source": [
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test_padded, y_test)\n",
    "print(\"LSTM Test Accuracy:\", lstm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7cbfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1031d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
